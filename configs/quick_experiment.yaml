# Quick experiment configuration for generating results
data:
  dataset_name: "ami"
  sample_rate: 16000
  segment_duration: 3.0
  hop_duration: 0.5
  batch_size: 16
  num_workers: 0
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  synthetic_data_size: 300
  n_fft: 512
  hop_length: 160
  n_mels: 80

model:
  embedding_dim: 128
  num_speakers: 4
  dropout: 0.3
  use_adaptive_refinement: true

boundary_refinement:
  enabled: true
  window_overlap: 0.5
  gradient_threshold: 0.15
  min_segment_duration: 0.2
  max_iterations: 3

training:
  num_epochs: 10
  learning_rate: 0.001
  weight_decay: 0.0001
  gradient_clip_norm: 1.0
  early_stopping_patience: 5
  checkpoint_dir: "checkpoints"
  best_model_path: "models/best_model.pt"
  mixed_precision: false

scheduler:
  type: "cosine"
  warmup_epochs: 1
  min_lr: 0.000001

loss:
  diarization_weight: 1.0
  boundary_consistency_weight: 0.5
  temporal_consistency_weight: 0.3
  speaker_confusion_penalty: 0.2

evaluation:
  collar: 0.25
  metrics:
    - diarization_error_rate
    - jaccard_error_rate
    - boundary_f1_score
    - speaker_confusion
  save_predictions: true
  results_dir: "results"

mlflow:
  enabled: false

random_seed: 42
